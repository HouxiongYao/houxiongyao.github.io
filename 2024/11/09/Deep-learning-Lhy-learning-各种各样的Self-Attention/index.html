<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/output32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/output16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Times New Roman:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="介绍各种各样的自注意力机制">
<meta property="og:type" content="article">
<meta property="og:title" content="各种各样的Self-Attention">
<meta property="og:url" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/index.html">
<meta property="og:site_name" content="熊熊学习乐园">
<meta property="og:description" content="介绍各种各样的自注意力机制">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-104635.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-104950.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-105543.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-095441.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-095818.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-100948.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-103731.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-104341.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-105059.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-111721.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-113144.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-110044.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-110807.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-111459.png">
<meta property="og:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-113521.png">
<meta property="article:published_time" content="2024-11-08T16:00:00.000Z">
<meta property="article:modified_time" content="2024-11-19T06:53:09.217Z">
<meta property="article:author" content="Houxiong">
<meta property="article:tag" content="deep-learning-notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-104635.png">

<link rel="canonical" href="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>各种各样的Self-Attention | 熊熊学习乐园</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">熊熊学习乐园</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">A place for growing</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/HouxiongYao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Houxiong">
      <meta itemprop="description" content="To learn, To copy">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="熊熊学习乐园">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          各种各样的Self-Attention
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-11-09 00:00:00" itemprop="dateCreated datePublished" datetime="2024-11-09T00:00:00+08:00">2024-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-19 14:53:09" itemprop="dateModified" datetime="2024-11-19T14:53:09+08:00">2024-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index"><span itemprop="name">Deep-learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/Lhy-learning/" itemprop="url" rel="index"><span itemprop="name">Lhy-learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>介绍各种各样的自注意力机制 <span id="more"></span></p>
<blockquote>
<p>[!NOTE] Notice Self-attention 往往只是一个network中的一个小模块，当input序列长度很长时，Self-attention的计算量很惊人，以至于成为整个模型的主要计算开销。</p>
</blockquote>
<h2 id="local-attentiontruncated-attention">Local Attention/Truncated Attention</h2>
<p>attention矩阵并非必须要全部计算，有一些值可以根据对任务的理解预设。</p>
<p>例如，有时对于一个序列，我们并非需要知道整个距离的attention，而只需要关注左右邻居，所以我们可以将除开左右邻居的所有attention weight设为0。 <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-104635.png"> 由于只看见了小范围的咨讯，所以这样做的self-Attention和CNN很相似了。 ## Stride Attention 设置一个stride，看远距离的几个邻居 <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-104950.png"> ## GLobal Attention 在原始的序列中加入一个特殊的符号，这个符号的作用：</p>
<ul>
<li>Attend to every token<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.181ex" role="img" focusable="false" viewbox="0 -511 1000 522"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"/></g></g></g></svg></mjx-container></span>collect global information</li>
<li>Attended by every token <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.181ex" role="img" focusable="false" viewbox="0 -511 1000 522"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"/></g></g></g></svg></mjx-container></span>it knows global information</li>
</ul>
<p>两种加入特殊token的做法：</p>
<ul>
<li>在原始序列中选择一个token，例如开始符号cls或者句号等等</li>
<li>外加token，下图的例子使用的token是句子的前两个token，特殊token和其他所有token对应的key都做了dot product，而non-special token之间没有attention。 <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241109-105543.png"></li>
</ul>
<blockquote>
<p>[!NOTE] Best in practice Different attention in different heads! ## Focus on Critical Parts</p>
</blockquote>
<p>We want to use data-driven methods instead of manual operations. In an attention matrix, the values of critical parts should be large, while small values in other parts (such as setting them directly to 0) will reduce their influence on the results. So, the question becomes: how can we quickly estimate portions with small attention weights? <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-095441.png"></p>
<h3 id="clustering">Clustering</h3>
<p>Both Reformer and Routing Transformer use clustering to address the above issue. - <strong>Step 1:</strong> Cluster the query and key based on similarity. Keys and queries in the same cluster will be calculated later. Clustering methods can be approximate and fast.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-095818.png"></p>
<ul>
<li><strong>Step 2:</strong> Only keys and queries in the same cluster will be used to calculate attention weights; otherwise, the attention weight is set to 0.</li>
</ul>
<h3 id="learnable-patterns">Learnable Patterns</h3>
<p><strong>Sinkhorn Sorting Network</strong> uses a learned module to decide if a grid should be skipped or not.</p>
<p>The matrix on the left contains binary values. The input sequence is passed through a neural network that generates vectors (each vector has the same length as the input sequence).</p>
<p>A critical part of the network is that it makes the matrix on the right, a non-binary one, similar to the matrix on the left. The process is differentiable.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-100948.png"></p>
<h2 id="do-we-need-the-full-attention-matrix">Do We Need the Full Attention Matrix?</h2>
<p>Linformer proposed that the attention matrix tends to be low-rank and contains many redundant columns, which makes computation inefficient. <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-103731.png"></p>
<h3 id="choose-representative-keys">Choose Representative Keys</h3>
<p>To construct the attention matrix, we choose representative keys. Additionally, we need to select representative vectors to obtain the outputs of self-attention.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-104341.png"></p>
<p>However, a question arises: Can we reduce the number of queries? This generally depends on the task. For sequence-to-label tasks, reducing the number of queries might be acceptable, as not all queries are needed to make predictions. But if we need to label every position in the query, such as assigning phoneme labels to each frame, reducing the number of queries will likely result in incorrect predictions.</p>
<h3 id="how-to-reduce-the-number-of-keys">How to Reduce the Number of Keys</h3>
<p>Compressed Attention uses convolution for subsampling, while Linformer uses <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="8.885ex" height="1.975ex" role="img" focusable="false" viewbox="0 -716 3927.1 873.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="TeXAtom" transform="translate(783,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g></g></g><g data-mml-node="msub" transform="translate(1828.6,0)"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"/></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g></g></g></g></g></svg></mjx-container></span> to produce an output matrix with shape <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="5.953ex" height="1.593ex" role="img" focusable="false" viewbox="0 -694 2631.4 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mo" transform="translate(742.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"/></g><g data-mml-node="mi" transform="translate(1742.4,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"/></g></g></g></svg></mjx-container></span>, where each vector in the output is a combination of the original <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewbox="0 -716 750 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g></g></g></svg></mjx-container></span> matrix.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-105059.png"></p>
<p>However, numerous matrix operations can be costly. We can reduce the operation cost by changing the order of operations.</p>
<p>The softmax process can generally be described as follows (a comprehensive and complex computation process will be introduced later). Pay attention to the blue and yellow vectors, which are not related to the annotation <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewbox="0 -666 500 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container></span> of <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.958ex" height="1.912ex" role="img" focusable="false" viewbox="0 -833.9 865.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mn" transform="translate(462,363) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g></svg></mjx-container></span>; this means we only need to calculate them once, and for each output, only the query needs to change.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-111721.png"></p>
<p>These papers show how to obtain the <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.348ex" height="2.034ex" role="img" focusable="false" viewbox="0 -694 596 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D719" d="M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"/></g></g></g></svg></mjx-container></span>.</p>
<p><img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-113144.png"></p>
<ul>
<li>Full computation process: <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-110044.png"> <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-110807.png"> <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-111459.png"></li>
</ul>
<h2 id="do-we-need-q-and-k-to-compute-attention-synthesizer">Do We Need q and k to Compute Attention? Synthesizer!</h2>
<p>In Synthesizer, we don't compute attention scores from q and k; instead, we regard the attention matrix as network parameters.</p>
<p>Now we need to rethink the meaning of self-attention… <img src="/2024/11/09/Deep-learning-Lhy-learning-%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84Self-Attention/image-20241110-113521.png"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deep-learning-notes/" rel="tag"># deep-learning-notes</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/11/07/Programming-Python-Pytorch-tensor-masked-fill/" rel="prev" title="tensor.masked_fill">
      <i class="fa fa-chevron-left"></i> tensor.masked_fill
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/11/11/Programming-Cpp-chapter9-Error-detection-and-Handling/" rel="next" title="Error-detection-and-Handling">
      Error-detection-and-Handling <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#local-attentiontruncated-attention"><span class="nav-number">1.</span> <span class="nav-text">Local Attention&#x2F;Truncated Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#clustering"><span class="nav-number">1.1.</span> <span class="nav-text">Clustering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learnable-patterns"><span class="nav-number">1.2.</span> <span class="nav-text">Learnable Patterns</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#do-we-need-the-full-attention-matrix"><span class="nav-number">2.</span> <span class="nav-text">Do We Need the Full Attention Matrix?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#choose-representative-keys"><span class="nav-number">2.1.</span> <span class="nav-text">Choose Representative Keys</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-reduce-the-number-of-keys"><span class="nav-number">2.2.</span> <span class="nav-text">How to Reduce the Number of Keys</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#do-we-need-q-and-k-to-compute-attention-synthesizer"><span class="nav-number">3.</span> <span class="nav-text">Do We Need q and k to Compute Attention? Synthesizer!</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Houxiong"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Houxiong</p>
  <div class="site-description" itemprop="description">To learn, To copy</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024-11 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Houxiong</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>


